{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50162421,  0.45199597,  0.76661218,  0.24734441,  0.50188487,\n",
       "        -0.30220332, -0.46014422,  0.79177244, -0.14381762],\n",
       "       [ 0.92968009,  0.326883  ,  0.24339144, -0.77050805,  0.89897852,\n",
       "        -0.10017573,  0.15677923, -0.18372639, -0.52594604],\n",
       "       [ 0.80675904,  0.14735897, -0.99425935,  0.23428983, -0.3467102 ,\n",
       "         0.0541162 ,  0.7718842 , -0.28546048,  0.8170703 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 9)\n",
    "\n",
    "np.vstack((2 * np.random.random(shape) - 1, 2 * np.random.random((1, shape[1])) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)  # Seed the random number generator\n",
    "        self.weights = {}  # Create dict to hold weights\n",
    "        self.num_layers = 1  # Set initial number of layer to one (input layer)\n",
    "        self.adjustments = {}  # Create dict to hold adjustements\n",
    "\n",
    "    def add_layer(self, shape):\n",
    "        # Create weights with shape specified + biases\n",
    "        self.weights[self.num_layers] = np.vstack((2 * np.random.random(shape) - 1, 2 * np.random.random((1, shape[1])) - 1))\n",
    "        # Initialize the adjustements for these weights to zero\n",
    "        self.adjustments[self.num_layers] = np.zeros(shape)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # Pass data through pretrained network\n",
    "        for layer in range(1, self.num_layers+1):\n",
    "            data = np.dot(data, self.weights[layer-1][:, :-1]) + self.weights[layer-1][:, -1] # + self.biases[layer]\n",
    "            data = self.__sigmoid(data)\n",
    "        return data\n",
    "    \n",
    "    def __forward_propagate(self, data):\n",
    "        # Progapagate through network and hold values for use in back-propagation\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        for layer in range(2, self.num_layers+1):\n",
    "            data = np.dot(data.T, self.weights[layer-1][:-1, :]) + self.weights[layer-1][-1, :].T # + self.biases[layer]\n",
    "            data = self.__sigmoid(data).T\n",
    "            activation_values[layer] = data\n",
    "        return activation_values\n",
    "\n",
    "    def simple_error(self, outputs, targets):\n",
    "        return targets - outputs\n",
    "\n",
    "    def sum_squared_error(self, outputs, targets):\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "\n",
    "    def __back_propagate(self, output, target):\n",
    "        deltas = {}\n",
    "        # Delta of output Layer\n",
    "        deltas[self.num_layers] = output[self.num_layers] - target\n",
    "\n",
    "        # Delta of hidden Layers\n",
    "        for layer in reversed(range(2, self.num_layers)):  # All layers except input/output\n",
    "            a_val = output[layer]\n",
    "            weights = self.weights[layer][:-1, :]\n",
    "            prev_deltas = deltas[layer+1]\n",
    "            deltas[layer] = np.multiply(np.dot(weights, prev_deltas), self.__sigmoid_derivative(a_val))\n",
    "\n",
    "        # Caclculate total adjustements based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "            self.adjustments[layer] += np.dot(deltas[layer+1], output[layer].T).T\n",
    "\n",
    "    def __gradient_descente(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        for layer in range(1, self.num_layers):\n",
    "            partial_d = (1/batch_size) * self.adjustments[layer]\n",
    "            self.weights[layer][:-1, :] += learning_rate * -partial_d\n",
    "            self.weights[layer][-1, :] += learning_rate*1e-3 * -partial_d[-1, :]\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=1, stop_accuracy=1e-5):\n",
    "        error = []\n",
    "        for iteration in range(num_epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                # Pass the training set through our neural network\n",
    "                output = self.__forward_propagate(x)\n",
    "\n",
    "                # Calculate the error\n",
    "                loss = self.sum_squared_error(output[self.num_layers], y)\n",
    "                error.append(loss)\n",
    "\n",
    "                # Calculate Adjustements\n",
    "                self.__back_propagate(output, y)\n",
    "\n",
    "            self.__gradient_descente(i, learning_rate)\n",
    "\n",
    "            # Check if accuarcy criterion is satisfied\n",
    "            if np.mean(error[-(i+1):]) < stop_accuracy and iteration > 0:\n",
    "                break\n",
    "\n",
    "        return(np.asarray(error), iteration+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Error = ', 0.12919882118364132)\n",
      "('Epoches needed to train = ', 5000)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ----------- XOR Function -----------------\n",
    "\n",
    "    # Create instance of a neural network\n",
    "    nn = NeuralNetwork()\n",
    "\n",
    "    # Add Layers (Input layer is created by default)\n",
    "    nn.add_layer((2, 9))\n",
    "    nn.add_layer((9, 1))\n",
    "\n",
    "    # XOR function\n",
    "    training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2, 1)\n",
    "    training_labels = np.asarray([[0], [1], [1], [0]])\n",
    "\n",
    "    error, iteration = nn.train(training_data, training_labels, 5000)\n",
    "    print('Error = ', np.mean(error[-4:]))\n",
    "    print('Epoches needed to train = ', iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights (layer 1): \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822]\n",
      " [-0.81532281 -0.62747958 -0.30887855 -0.20646505  0.07763347]\n",
      " [-0.16161097  0.370439   -0.5910955   0.75623487 -0.94522481]]\n",
      "\n",
      "Random starting synaptic weights (layer 2): \n",
      "[[ 0.34093502 -0.1653904   0.11737966 -0.71922612]\n",
      " [-0.60379702  0.60148914  0.93652315 -0.37315164]\n",
      " [ 0.38464523  0.7527783   0.78921333 -0.82991158]\n",
      " [-0.92189043 -0.66033916  0.75628501 -0.80330633]\n",
      " [-0.15778475  0.91577906  0.06633057  0.38375423]]\n",
      "\n",
      "Random starting synaptic weights (layer 3): \n",
      "[[-0.36896874]\n",
      " [ 0.37300186]\n",
      " [ 0.66925134]\n",
      " [-0.96342345]]\n",
      "\n",
      "New synaptic weights (layer 1) after training: \n",
      "[[-0.39042717  4.02220543 -1.52322523  2.40451717 -2.77177632]\n",
      " [-0.86817904 -0.33659723 -0.245578   -0.31292608  0.26079733]\n",
      " [-0.00600591 -1.69046817  0.12647375 -0.79367455  1.04614   ]]\n",
      "\n",
      "New synaptic weights (layer 2) after training: \n",
      "[[ 0.9614375  -0.15372521 -0.67703076 -0.00498486]\n",
      " [-2.7714058   0.77362787  2.71638353 -2.4249225 ]\n",
      " [ 1.88550044  0.70717346 -0.71729366  0.7730995 ]\n",
      " [-1.59473372 -0.55756571  1.23221965 -1.28695185]\n",
      " [ 1.92232578  0.86077523 -2.13676866  2.54238247]]\n",
      "\n",
      "New synaptic weights (layer 3) after training: \n",
      "[[-4.392069  ]\n",
      " [ 0.66563256]\n",
      " [ 5.76280212]\n",
      " [-3.88936424]]\n",
      "\n",
      "Considering new situation [1,0,0] -> ?\n",
      "[ 0.99650838]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "\n",
    "class NeuralNetwork():\n",
    "\tdef __init__(self):\n",
    "\t\trandom.seed(1)\n",
    "\n",
    "\t\t# setting the number of nodes in layer 2 and layer 3\n",
    "\t\t# more nodes --> more confidence in predictions (?)\n",
    "\t\tl2 = 5\n",
    "\t\tl3 = 4\n",
    "\n",
    "\t\t# assign random weights to matrices in network\n",
    "\t\t# format is (no. of nodes in previous layer) x (no. of nodes in following layer)\n",
    "\t\tself.synaptic_weights1 = 2 * random.random((3, l2)) -1\n",
    "\t\tself.synaptic_weights2 = 2 * random.random((l2, l3)) -1\n",
    "\t\tself.synaptic_weights3 = 2 * random.random((l3, 1)) -1\n",
    "\t\t\n",
    "\tdef __sigmoid(self, x):\n",
    "\t\treturn 1/(1+exp(-x))\n",
    "\n",
    "\t# derivative of sigmoid function, indicates confidence about existing weight\n",
    "\tdef __sigmoid_derivative(self, x):\n",
    "\t\treturn x*(1-x)\n",
    "\n",
    "\t# train neural network, adusting synaptic weights each time\n",
    "\tdef train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "\t\tfor iteration in xrange(number_of_training_iterations):\n",
    "\n",
    "\t\t\t# pass training set through our neural network\n",
    "\t\t\t# a2 means the activations fed to second layer\n",
    "\t\t\ta2 = self.__sigmoid(dot(training_set_inputs, self.synaptic_weights1))\n",
    "\t\t\ta3 = self.__sigmoid(dot(a2, self.synaptic_weights2))\n",
    "\t\t\toutput = self.__sigmoid(dot(a3, self.synaptic_weights3))\n",
    "\n",
    "\t\t\t# calculate 'error'\n",
    "\t\t\tdel4 = (training_set_outputs - output)*self.__sigmoid_derivative(output)\n",
    "\n",
    "\t\t\t# find 'errors' in each layer\n",
    "\t\t\tdel3 = dot(self.synaptic_weights3, del4.T)*(self.__sigmoid_derivative(a3).T)\n",
    "\t\t\tdel2 = dot(self.synaptic_weights2, del3)*(self.__sigmoid_derivative(a2).T)\n",
    "\n",
    "\t\t\t# get adjustments (gradients) for each layer\n",
    "\t\t\tadjustment3 = dot(a3.T, del4)\n",
    "\t\t\tadjustment2 = dot(a2.T, del3.T)\n",
    "\t\t\tadjustment1 = dot(training_set_inputs.T, del2.T)\n",
    "\n",
    "\t\t\t# adjust weights accordingly\n",
    "\t\t\tself.synaptic_weights1 += adjustment1\n",
    "\t\t\tself.synaptic_weights2 += adjustment2\n",
    "\t\t\tself.synaptic_weights3 += adjustment3\n",
    "\n",
    "\tdef forward_pass(self, inputs):\n",
    "\t\t# pass our inputs through our neural network\n",
    "\t\ta2 = self.__sigmoid(dot(inputs, self.synaptic_weights1))\n",
    "\t\ta3 = self.__sigmoid(dot(a2, self.synaptic_weights2))\n",
    "\t\toutput = self.__sigmoid(dot(a3, self.synaptic_weights3)) \n",
    "\t\treturn output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# initialise single neuron neural network\n",
    "\tneural_network = NeuralNetwork()\n",
    "\n",
    "\tprint \"Random starting synaptic weights (layer 1): \"\n",
    "\tprint neural_network.synaptic_weights1\n",
    "\tprint \"\\nRandom starting synaptic weights (layer 2): \"\n",
    "\tprint neural_network.synaptic_weights2\n",
    "\tprint \"\\nRandom starting synaptic weights (layer 3): \"\n",
    "\tprint neural_network.synaptic_weights3\n",
    "\n",
    "\t# the training set.\n",
    "\ttraining_set_inputs = array([[0,0,1],[1,1,1],[1,0,1],[0,1,1]])\n",
    "\ttraining_set_outputs = array([[0,1,1,0]]).T\n",
    "\n",
    "\tneural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "\tprint \"\\nNew synaptic weights (layer 1) after training: \"\n",
    "\tprint neural_network.synaptic_weights1\n",
    "\tprint \"\\nNew synaptic weights (layer 2) after training: \"\n",
    "\tprint neural_network.synaptic_weights2\n",
    "\tprint \"\\nNew synaptic weights (layer 3) after training: \"\n",
    "\tprint neural_network.synaptic_weights3\n",
    "\n",
    "\t# test with new input\n",
    "\tprint \"\\nConsidering new situation [1,0,0] -> ?\"\n",
    "\tprint neural_network.forward_pass(array([1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b62d233dac99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
